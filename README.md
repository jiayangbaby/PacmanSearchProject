Overview: 
This project implements search algorithms to guide Pac-Man through a maze to reach a goal state. Each search strategy demonstrates different approaches to pathfinding in an AI context.

Algorithms Implemented

Project 1
Depth-First Search (DFS): Explores as far as possible along each branch before backtracking.
Breadth-First Search (BFS): Explores all neighbors at the present depth level before moving to deeper nodes.

Project 2
Reflex Agent: A basic agent that makes decisions based on the current state.
Minimax Agent: Implements adversarial search to consider both Pacman and ghosts' moves.
Alpha-Beta Pruning: An optimized version of Minimax to prune unnecessary branches and speed up the search.
Expectimax Agent: A probabilistic search to model the behavior of random or suboptimal ghost agents.

Project 3
Value Iteration: iteratively updates state values to find the optimal policy in a Markov Decision Process.
Q-Learning: A model-free reinforcement learning algorithm that updates the value of state-action pairs to learn the best actions for each state.
Epsilon Greedy: balances exploration (random actions) and exploitation (best-known actions) by selecting random actions with probability 
Bridge Crossing Revisited: A problem where an optimal strategy is needed for a group of people to cross a bridge in the least time, often solved using reinforcement learning methods.
Approximate Q-Learning: A variant of Q-learning that uses function approximation (e.g., neural networks) to handle large state or action spaces instead of a Q-table.


Acknowledgments
The projects were developed by John DeNero, Dan Klein, Pieter Abbeel, and many others.

